{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d662d9a3",
   "metadata": {},
   "source": [
    "#  Person Vehicle Bike Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eae42066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbconvert --clear-output --inplace  215-person-vehicle-bike-detection.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fec439bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215-person-vehicle-bike-detection.ipynb:cell_1:1:1: E265 block comment should start with '# '\n",
      "215-person-vehicle-bike-detection.ipynb:cell_1:1:80: E501 line too long (85 > 79 characters)\n",
      "215-person-vehicle-bike-detection.ipynb:cell_6:7:25: W291 trailing whitespace\n",
      "215-person-vehicle-bike-detection.ipynb:cell_6:8:80: E501 line too long (101 > 79 characters)\n",
      "215-person-vehicle-bike-detection.ipynb:cell_6:9:2: E225 missing whitespace around operator\n",
      "215-person-vehicle-bike-detection.ipynb:cell_7:3:1: E302 expected 2 blank lines, found 0\n",
      "215-person-vehicle-bike-detection.ipynb:cell_8:2:80: E501 line too long (82 > 79 characters)\n",
      "215-person-vehicle-bike-detection.ipynb:cell_9:1:1: E265 block comment should start with '# '\n",
      "215-person-vehicle-bike-detection.ipynb:cell_10:2:1: E265 block comment should start with '# '\n",
      "215-person-vehicle-bike-detection.ipynb:cell_10:3:1: E265 block comment should start with '# '\n",
      "215-person-vehicle-bike-detection.ipynb:cell_12:1:80: E501 line too long (88 > 79 characters)\n",
      "215-person-vehicle-bike-detection.ipynb:cell_12:2:68: W291 trailing whitespace\n",
      "215-person-vehicle-bike-detection.ipynb:cell_12:3:1: E265 block comment should start with '# '\n",
      "215-person-vehicle-bike-detection.ipynb:cell_12:4:80: E501 line too long (94 > 79 characters)\n",
      "215-person-vehicle-bike-detection.ipynb:cell_12:8:80: E501 line too long (91 > 79 characters)\n",
      "215-person-vehicle-bike-detection.ipynb:cell_12:17:80: E501 line too long (92 > 79 characters)\n",
      "215-person-vehicle-bike-detection.ipynb:cell_12:18:74: W291 trailing whitespace\n",
      "215-person-vehicle-bike-detection.ipynb:cell_12:19:80: E501 line too long (80 > 79 characters)\n",
      "215-person-vehicle-bike-detection.ipynb:cell_12:19:81: W291 trailing whitespace\n",
      "215-person-vehicle-bike-detection.ipynb:cell_12:20:80: E501 line too long (94 > 79 characters)\n",
      "215-person-vehicle-bike-detection.ipynb:cell_12:20:95: W291 trailing whitespace\n",
      "215-person-vehicle-bike-detection.ipynb:cell_12:21:17: E128 continuation line under-indented for visual indent\n",
      "215-person-vehicle-bike-detection.ipynb:cell_12:23:42: W291 trailing whitespace\n",
      "215-person-vehicle-bike-detection.ipynb:cell_12:24:13: E265 block comment should start with '# '\n",
      "215-person-vehicle-bike-detection.ipynb:cell_12:24:80: E501 line too long (98 > 79 characters)\n",
      "215-person-vehicle-bike-detection.ipynb:cell_12:25:80: E501 line too long (100 > 79 characters)\n",
      "215-person-vehicle-bike-detection.ipynb:cell_12:27:80: E501 line too long (135 > 79 characters)\n",
      "215-person-vehicle-bike-detection.ipynb:cell_13:1:1: E265 block comment should start with '# '\n",
      "215-person-vehicle-bike-detection.ipynb:cell_13:4:80: E501 line too long (88 > 79 characters)\n"
     ]
    }
   ],
   "source": [
    "# nbqa\n",
    "!nbqa flake8 215-person-vehicle-bike-detection.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ade7d4a",
   "metadata": {},
   "source": [
    "# 1.Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a703c20",
   "metadata": {},
   "outputs": [],
   "source": [
    " Importing basic packages required\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from openvino.runtime import Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42111c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path.exists??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6091fc24",
   "metadata": {},
   "source": [
    "# 2.Download Models\n",
    "\n",
    "We need to download pretrained models to continue our progress.We use omz_downloader, a command-line tool installed by openvino-dev package.\n",
    "\n",
    "Note: If you want to change the model,you need to modify the model name,such as \"person-vehicle-bike-detection-2003\",\"vehicle-detection-0202\" .They support different image input sizes in detections.If you want to change the precision,you need to modify the precision value in \"FP32\", \"FP16\", \"FP16-INT8\",different type has different model size and precision value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd3d142",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"person-vehicle-bike-detection-2003\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ac937c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where model will be downloaded\n",
    "base_model_dir = \"model\"\n",
    "# Model name as named in Open Model Zoo\n",
    "detection_model_name = \"person-vehicle-bike-detection-2003\"\n",
    "# Selected precision (FP32, FP16, FP16-INT8)\n",
    "precision = \"FP32\"\n",
    "# Check the model exists \n",
    "detection_model_path = (f\"model/intel/{detection_model_name}/{precision}/{detection_model_name}.xml\")\n",
    "p=Path(detection_model_path)\n",
    "# Download detection model\n",
    "if not p.exists():\n",
    "    download_command = f\"omz_downloader \" \\\n",
    "                       f\"--name {detection_model_name} \" \\\n",
    "                       f\"--precision {precision} \" \\\n",
    "                       f\"--output_dir {base_model_dir}\"\n",
    "    ! $download_command\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf3d4fe",
   "metadata": {},
   "source": [
    "# 3.Load Models\n",
    "In this notebook,we will need detection model and recognition model.When we download models,we need to initialize inference engine(IECore),and use read_network to read network architecture and weights from .xml and .bin files.Then,we load the network on the \"CPU\" using load_network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e560c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize inference engine\n",
    "ie_core = Core()\n",
    "def model_init(model_path: str):\n",
    "    \"\"\"\n",
    "    Read the network and weights from file, load the\n",
    "    model on the CPU and get input and output names of nodes\n",
    "\n",
    "    :param: model: model architecture path *.xml\n",
    "    :retuns:\n",
    "            input_key: Input node network\n",
    "            output_key: Output node network\n",
    "            exec_net: Encoder model network\n",
    "            net: Model network\n",
    "    \"\"\"\n",
    "    # Read the network and corresponding weights from file\n",
    "    model = ie_core.read_model(model=model_path)\n",
    "    # compile the model for the CPU (you can use GPU or MYRIAD as well)\n",
    "    compiled_model = ie_core.compile_model(model=model, device_name=\"CPU\")\n",
    "    # Get input and output names of nodes\n",
    "    input_keys = compiled_model.inputs[0]\n",
    "    output_keys = compiled_model.outputs[0]\n",
    "    return input_keys, output_keys, compiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f628561",
   "metadata": {},
   "source": [
    "### 3.1 Get attributes from model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29667ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection model initialization\n",
    "input_key_de, output_keys_de, compiled_model_de = model_init(detection_model_path)\n",
    "# Get input size - Detection\n",
    "height_de, width_de = list(input_key_de.shape)[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1300bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for showing Raw Image\n",
    "def plt_img(raw_image):\n",
    "    \"\"\"\n",
    "    Use matplot to show image inline\n",
    "    raw_image: input image\n",
    "\n",
    "    :param: raw_image:image array\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(raw_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6279f26e",
   "metadata": {},
   "source": [
    "####  3.2.Read and show a test image\n",
    "From detection model input shape [1, 3, 480, 864],we need to resize the image size to 864x480 resolution.,and expand batch channel with expand_dims function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b390bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read a image\n",
    "#image_de = cv2.imread(\"./data/person-vehicle-bike-detection-2003.png\")\n",
    "#image_de = cv2.imread(\"./data/park.jpg\")\n",
    "image_de = cv2.imread(\"./data/nyu.jpg\")\n",
    "# Resize to [3, 256, 256]\n",
    "resized_image_de = cv2.resize(image_de, (width_de, height_de))\n",
    "# Expand to [1, 3, 256, 256]\n",
    "input_image_de = np.expand_dims(resized_image_de.transpose(2, 0, 1), 0)\n",
    "# Show image\n",
    "plt_img(raw_image=cv2.cvtColor(image_de, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b97bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference request\n",
    "request = compiled_model_de.create_infer_request()\n",
    "request.infer({input_key_de.any_name: input_image_de})\n",
    "boxes = request.get_tensor(\"boxes\").data\n",
    "# Remove zero only boxes\n",
    "boxes = boxes[~np.all(boxes == 0, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d1ce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each detection, the description has the format: [x_min, y_min, x_max, y_max, conf]\n",
    "# Image passed here is in BGR format with changed width and height. \n",
    "#To display it in colors expected by matplotlib we use cvtColor function\n",
    "def convert_result_to_image(bgr_image, resized_image, boxes, threshold=0.3, conf_labels=True):\n",
    "    # Define colors for boxes and descriptions\n",
    "    colors = {\"red\": (255, 0, 0), \"green\": (0, 255, 0)}\n",
    "    # Fetch image shapes to calculate ratio\n",
    "    (real_y, real_x), (resized_y, resized_x) = bgr_image.shape[:2], resized_image.shape[:2]\n",
    "    ratio_x, ratio_y = real_x / resized_x, real_y / resized_y\n",
    "    # Convert base image from bgr to rgb format\n",
    "    rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
    "    # Iterate through non-zero boxes\n",
    "    for box in boxes:\n",
    "        # Pick confidence factor from last place in array\n",
    "        conf = box[-1]\n",
    "        if conf > threshold:\n",
    "            # Convert float to int and multiply corner position of each box by x and y ratio\n",
    "            # In case that bounding box is found at the top of the image, \n",
    "            # we position upper box bar little lower to make it visible on image \n",
    "            (x_min, y_min, x_max, y_max) = [int(max(corner_position * ratio_y, 10)) if idx % 2 \n",
    "                else int(corner_position * ratio_x)\n",
    "                for idx, corner_position in enumerate(box[:-1])]\n",
    "            # Draw box based on position, \n",
    "            #parameters in rectangle function are: image, start_point, end_point, color, thickness\n",
    "            rgb_image = cv2.rectangle(rgb_image, (x_min, y_min), (x_max, y_max), colors[\"green\"], 3)\n",
    "            # Add text to image based on position and confidence\n",
    "            # Parameters in text function are: image, text, bottom-left_corner_textfield, font, font_scale, color, thickness, line_type\n",
    "            if conf_labels:\n",
    "                rgb_image = cv2.putText(\n",
    "                    rgb_image,\n",
    "                    f\"{conf:.2f}\",\n",
    "                    (x_min, y_min - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.8,\n",
    "                    colors[\"red\"],\n",
    "                    1,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "    return rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49686b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detected Image\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(convert_result_to_image(image_de, resized_image_de, boxes, conf_labels=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvine_env",
   "language": "python",
   "name": "openvine_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
